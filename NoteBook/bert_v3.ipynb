{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 1: **Load the Dataset**\n",
    "We’ll start by loading the dataset from the CSV file (`smsDataLast.csv`) using `pandas`.\n",
    "\n",
    "### Explanation:\n",
    "- The dataset contains SMS messages and their corresponding categories (labels).\n",
    "- We’ll use `pandas` to load the data into a DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../Data/labeled_combined_sms_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Message Content</th>\n",
       "      <th>category_labels</th>\n",
       "      <th>is_important</th>\n",
       "      <th>is_spam</th>\n",
       "      <th>transactions</th>\n",
       "      <th>dates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>samba.</td>\n",
       "      <td>تم خصم مبلغ ٥٠٫٠٠ من حساب ******٤٥٢١ في ٠٥-٠٥-...</td>\n",
       "      <td>['Money/Financial', 'Expense']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>{'amount': '٥٠٫٠٠', 'type': 'expense', 'accoun...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607941</td>\n",
       "      <td>Your WhatsApp code is 614-968 but you can simp...</td>\n",
       "      <td>['Other']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neqaty</td>\n",
       "      <td>Dear Member, you have not redeemed any Neqaty ...</td>\n",
       "      <td>['Promotion', 'Advertising']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neqaty</td>\n",
       "      <td>عزيزي العميل، لقد مر 17 شهر و لم تقم باي عملية...</td>\n",
       "      <td>['Notification', 'Promotion']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>606006</td>\n",
       "      <td>BIG SAVINGS! Now get 200 Mobily Minutes, 200 M...</td>\n",
       "      <td>['Promotion', 'Advertising']</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sender                                    Message Content  \\\n",
       "0  samba.  تم خصم مبلغ ٥٠٫٠٠ من حساب ******٤٥٢١ في ٠٥-٠٥-...   \n",
       "1  607941  Your WhatsApp code is 614-968 but you can simp...   \n",
       "2  neqaty  Dear Member, you have not redeemed any Neqaty ...   \n",
       "3  neqaty  عزيزي العميل، لقد مر 17 شهر و لم تقم باي عملية...   \n",
       "4  606006  BIG SAVINGS! Now get 200 Mobily Minutes, 200 M...   \n",
       "\n",
       "                  category_labels  is_important  is_spam  \\\n",
       "0  ['Money/Financial', 'Expense']             1        0   \n",
       "1                       ['Other']             0        0   \n",
       "2    ['Promotion', 'Advertising']             0        1   \n",
       "3   ['Notification', 'Promotion']             0        1   \n",
       "4    ['Promotion', 'Advertising']             0        1   \n",
       "\n",
       "                                        transactions dates  \n",
       "0  {'amount': '٥٠٫٠٠', 'type': 'expense', 'accoun...   NaN  \n",
       "1                                                NaN   NaN  \n",
       "2                                                NaN   NaN  \n",
       "3                                                NaN   NaN  \n",
       "4                                                NaN   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6996 entries, 0 to 6995\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Sender           6996 non-null   object\n",
      " 1   Message Content  6996 non-null   object\n",
      " 2   category_labels  6996 non-null   object\n",
      " 3   is_important     6996 non-null   int64 \n",
      " 4   is_spam          6996 non-null   int64 \n",
      " 5   transactions     3053 non-null   object\n",
      " 6   dates            484 non-null    object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 382.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_labels\n",
       "['Money/Financial', 'Expense']                       2219\n",
       "['Promotion', 'Advertising']                         1240\n",
       "['Other']                                             851\n",
       "['Notification', 'Other']                             361\n",
       "['Money/Financial', 'Expense', 'Notification']        346\n",
       "                                                     ... \n",
       "['Money/Financial', 'Notification', 'Government']       1\n",
       "['Money/Financial', 'Donation']                         1\n",
       "['Promotion', 'Investment']                             1\n",
       "['Money/Financial', 'Transaction']                      1\n",
       "['Promotion', 'Health', 'Education']                    1\n",
       "Name: count, Length: 116, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of categories\n",
    "df['category_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 2: **Preprocess the Data**\n",
    "We’ll preprocess the data to prepare it for training. This includes:\n",
    "- Mapping categories to numerical labels.\n",
    "- Splitting the data into training and testing sets.\n",
    "\n",
    "### Explanation:\n",
    "- Text classification requires numerical labels, so we’ll convert the `Category` column to numerical values.\n",
    "- We’ll split the data into training and testing sets (e.g., 80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Money/Financial', 'Expense']\n",
      "['Other']\n",
      "['Promotion', 'Advertising']\n",
      "['Notification', 'Promotion']\n",
      "['Money/Financial', 'Income']\n",
      "['Notification']\n",
      "['Notification', 'Health']\n",
      "['Advertising']\n",
      "['Notification', 'Other']\n",
      "['Money/Financial', 'Expense', 'Notification']\n",
      "['Notification', 'Promotion', 'Advertising']\n",
      "['Notification', 'Advertising']\n",
      "['Advertising', 'Promotion']\n",
      "['Money/Financial', 'Expense', 'Other']\n",
      "['Notification', 'Education']\n",
      "['Promotion', 'Education']\n",
      "['Other', 'Warning']\n",
      "['Government', 'Notification']\n",
      "['Education']\n",
      "['Notification', 'Education', 'Test/Exam']\n",
      "['Advertising', 'Other']\n",
      "['Health', 'Promotion', 'Advertising']\n",
      "['Notification', 'Test/Exam']\n",
      "['Health']\n",
      "['Promotion', 'Other']\n",
      "['Education', 'Notification']\n",
      "['Notification', 'Security']\n",
      "['Health', 'Promotion']\n",
      "['Notification', 'Appointment']\n",
      "['Notification', 'Event']\n",
      "['Education', 'Promotion']\n",
      "['Notification', 'Government']\n",
      "['Notification', 'Test/Exam', 'Education']\n",
      "['Health', 'Other']\n",
      "['Education', 'Advertising']\n",
      "['Government', 'Other']\n",
      "['Promotion', 'Notification']\n",
      "['Health', 'Government']\n",
      "['Notification', 'Travel']\n",
      "['Money/Financial', 'Expense', 'Travel']\n",
      "['Promotion', 'Notification', 'Expense']\n",
      "['Money/Financial', 'Income', 'Advertising']\n",
      "['Money/Financial', 'Expense', 'Promotion']\n",
      "['Money/Financial', 'Expense', 'Advertising']\n",
      "['Money/Financial', 'Notification']\n",
      "['Education', 'Promotion', 'Advertising']\n",
      "['Money/Financial']\n",
      "['Notification', 'Health', 'Appointment']\n",
      "['Money/Financial', 'Income', 'Promotion']\n",
      "['Money/Financial', 'Expense', 'Notification', 'Advertising']\n",
      "['Notification', 'Advertising', 'Promotion']\n",
      "['Promotion', 'Advertising', 'Notification']\n",
      "['Money/Financial', 'Promotion', 'Advertising']\n",
      "['Money/Financial', 'Expense', 'Promotion', 'Advertising']\n",
      "['Money/Financial', 'Notification', 'Promotion']\n",
      "['Money/Financial', 'Investments']\n",
      "['Advertising', 'Notification']\n",
      "['Money/Financial', 'Savings']\n",
      "['Government', 'Notification', 'Test/Exam']\n",
      "['Notification', 'Delivery']\n",
      "['Money/Financial', 'Promotion']\n",
      "['Notification', 'Government', 'Education']\n",
      "['Notification', 'Government', 'Other']\n",
      "['Notification', 'Travel', 'Other']\n",
      "['Notification', 'Promotion', 'Other']\n",
      "['Notification', 'Money/Financial']\n",
      "['Health', 'Notification']\n",
      "['Promotion', 'Travel']\n",
      "['Notification', 'Education', 'Appointment']\n",
      "['Advertising', 'Government']\n",
      "['Promotion', 'Education', 'Health']\n",
      "['Government', 'Health', 'Notification']\n",
      "['Advertising', 'Health']\n",
      "['Health', 'Government', 'Other']\n",
      "['Other', 'Notification']\n",
      "['Promotion', 'Government']\n",
      "['Health', 'Advertising']\n",
      "['Health', 'Notification', 'Advertising']\n",
      "['Government', 'Health', 'Notification', 'Advertising']\n",
      "['Health', 'Government', 'Notification']\n",
      "['Government', 'Health']\n",
      "['Notification', 'Health', 'Test/Exam']\n",
      "['Health', 'Notification', 'Test/Exam']\n",
      "['Advertising', 'Education']\n",
      "['Education', 'Other']\n",
      "['Government', 'Notification', 'Other']\n",
      "['Other', 'Advertising']\n",
      "['Government', 'Advertising']\n",
      "['Notification', 'Emergency']\n",
      "['Health', 'Notification', 'Education']\n",
      "['Money/Financial', 'Expense', 'Government']\n",
      "['Promotion', 'Advertising', 'Government']\n",
      "['Notification', 'Money/Financial', 'Loans']\n",
      "['Travel', 'Notification']\n",
      "['Government']\n",
      "['Promotion', 'Notification', 'Event']\n",
      "['Promotion', 'Travel', 'Advertising']\n",
      "['Travel', 'Promotion']\n",
      "['Money/Financial', 'Investment']\n",
      "['Government', 'Notification', 'Money/Financial', 'Expense']\n",
      "['Notification', 'Appointment', 'Education']\n",
      "['Money/Financial', 'Expense', 'Notification', 'Government']\n",
      "['Money/Financial', 'Notification', 'Government']\n",
      "['Notification', 'Promotion', 'Travel']\n",
      "['Notification', 'Travel', 'Promotion']\n",
      "['Promotion', 'Notification', 'Education']\n",
      "['Promotion', 'Advertising', 'Travel']\n",
      "['Education', 'Notification', 'Advertising']\n",
      "['Money/Financial', 'Income', 'Notification']\n",
      "['Money/Financial', 'Transfer']\n",
      "['Health', 'Notification', 'Government']\n",
      "['Money/Financial', 'Donation']\n",
      "['Promotion', 'Investment']\n",
      "['Money/Financial', 'Transaction']\n",
      "['Promotion', 'Event']\n",
      "['Promotion', 'Health', 'Education']\n"
     ]
    }
   ],
   "source": [
    "uniq_cat = df['category_labels'].unique()\n",
    "\n",
    "for i in uniq_cat:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['category_labels'] = df['category_labels'].apply(ast.literal_eval)\n",
    "df['Category'] = df['category_labels'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Money/Financial', 'Other', 'Promotion', 'Notification',\n",
       "       'Advertising', 'Government', 'Education', 'Health', 'Travel'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categories to numerical labels\n",
    "df['label'] = df['Category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      " Category\n",
      "Money/Financial    2530\n",
      "Notification       1045\n",
      "Promotion          1036\n",
      "Other               685\n",
      "Government          137\n",
      "Advertising          78\n",
      "Health               51\n",
      "Education            32\n",
      "Travel                2\n",
      "Name: count, dtype: int64\n",
      "Testing set class distribution:\n",
      " Category\n",
      "Money/Financial    633\n",
      "Notification       262\n",
      "Promotion          259\n",
      "Other              171\n",
      "Government          35\n",
      "Advertising         19\n",
      "Health              13\n",
      "Education            8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2,  # 20% of the data for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    shuffle=True,  # Shuffle the data before splitting\n",
    "    stratify=df['Category']  # Preserve the class distribution\n",
    ")\n",
    "\n",
    "# Check the distribution of categories in the training and testing sets\n",
    "print(\"Training set class distribution:\\n\", train_df['Category'].value_counts())\n",
    "print(\"Testing set class distribution:\\n\", test_df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (5596, 9)\n",
      "Testing data shape: (1400, 9)\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Testing data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 3: **Tokenize the Data**\n",
    "We’ll tokenize the text data using the `AutoTokenizer` from Hugging Face’s `transformers` library.\n",
    "\n",
    "### Explanation:\n",
    "- Tokenization converts text into numerical input that the model can understand.\n",
    "- We’ll use the `AutoTokenizer` for ModernBERT and ensure all sequences are padded/truncated to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Message Content\"], padding=\"max_length\", truncation=True, max_length=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the training and testing datasets\n",
    "train_dataset = train_df.apply(tokenize_function, axis=1)\n",
    "test_dataset = test_df.apply(tokenize_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized example: {'input_ids': [50281, 8181, 5843, 45232, 30265, 5843, 13504, 13621, 4467, 50011, 209, 149, 100, 149, 243, 149, 106, 149, 243, 149, 243, 31461, 40913, 14585, 47931, 209, 27591, 149, 99, 149, 100, 149, 97, 149, 96, 37710, 209, 149, 243, 149, 100, 14, 149, 243, 149, 100, 14, 149, 97, 149, 243, 50282], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check the tokenized output for the first example\n",
    "print(\"Tokenized example:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: **Test the Model on a Single Row**\n",
    "Before training, we’ll test the model on a single row to ensure it works.\n",
    "\n",
    "### Explanation:\n",
    "- We’ll load the ModernBERT model and pass a single tokenized input to it.\n",
    "- This helps verify that the model and tokenizer are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=len(df['Category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single row\n",
    "sample_input = tokenizer(train_df.iloc[0][\"Message Content\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=52)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class\n",
    "predicted_class_id = outputs.logits.argmax().item()\n",
    "predicted_class = df['Category'].unique()[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Health\n",
      "Actual class: Notification\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"Actual class:\", train_df.iloc[0][\"Category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: **Train the Model**\n",
    "We’ll train the model using the `Trainer` API from Hugging Face.\n",
    "\n",
    "### Explanation:\n",
    "- The `Trainer` API simplifies the training process by handling training loops, evaluation, and logging.\n",
    "- We’ll define training arguments (e.g., learning rate, batch size) and train the model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52c6615b2cc4ca99fdae473158b37cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5d1c6c438a42fe9f9ecca70109ef80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mana-saleh/anaconda3/envs/bert_env/lib/python3.12/site-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    logging_strategy=\"epoch\",     # Log metrics at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",         # Directory for logs\n",
    "    report_to=\"all\",              # Log to all available trackers (e.g., TensorBoard, W&B)\n",
    "    load_best_model_at_end=True,  # Required for EarlyStoppingCallback\n",
    "    metric_for_best_model=\"eval_loss\",  # Use validation loss to determine the best model\n",
    "    greater_is_better=False,      # Lower validation loss is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop if validation loss doesn't improve for 3 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mana-saleh/anaconda3/envs/bert_env/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='7000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/7000 10:21 < 24:12, 3.37 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.444736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364900</td>\n",
       "      <td>0.387091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>0.371512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.395151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.116700</td>\n",
       "      <td>0.458683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.587531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2100, training_loss=0.26606875374203637, metrics={'train_runtime': 625.9922, 'train_samples_per_second': 178.788, 'train_steps_per_second': 11.182, 'total_flos': 1162061943037632.0, 'train_loss': 0.26606875374203637, 'epoch': 6.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 6: **Evaluate the Model**\n",
    "After training, we’ll evaluate the model on the test set.\n",
    "\n",
    "### Explanation:\n",
    "- We’ll use the `Trainer` API to evaluate the model’s performance on the test dataset.\n",
    "- Metrics like accuracy, precision, recall, and F1 score can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.3715115785598755, 'eval_runtime': 5.9726, 'eval_samples_per_second': 234.403, 'eval_steps_per_second': 29.3, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8878571428571429\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.67      0.11      0.18        19\n",
      "     Class 1       0.00      0.00      0.00         8\n",
      "     Class 2       0.53      0.71      0.61        35\n",
      "     Class 3       0.62      0.38      0.48        13\n",
      "     Class 4       0.98      0.98      0.98       633\n",
      "     Class 5       0.83      0.83      0.83       262\n",
      "     Class 6       0.79      0.84      0.81       171\n",
      "     Class 7       0.88      0.88      0.88       259\n",
      "\n",
      "    accuracy                           0.89      1400\n",
      "   macro avg       0.66      0.59      0.60      1400\n",
      "weighted avg       0.89      0.89      0.88      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get predictions for the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = test_dataset['label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Get unique classes in true_labels\n",
    "unique_classes = np.unique(true_labels)\n",
    "\n",
    "# Ensure target_names matches the unique classes\n",
    "target_names = [f\"Class {i}\" for i in unique_classes]  # Replace with actual class names if available\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(\n",
    "    true_labels, \n",
    "    predicted_labels, \n",
    "    target_names=target_names, \n",
    "    labels=unique_classes  # Ensure alignment between labels and target_names\n",
    ")\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 7: **Save the Model**\n",
    "We’ll save the fine-tuned model for future use.\n",
    "\n",
    "### Explanation:\n",
    "- Saving the model allows us to reuse it without retraining.\n",
    "- We’ll save both the model and tokenizer to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"../Models/fine-tuned-modernbert_v2\")\n",
    "tokenizer.save_pretrained(\"../Models/fine-tuned-modernbert_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 8: **Load the Fine-Tuned Model**\n",
    "To load the fine-tuned model later:\n",
    "\n",
    "### Explanation:\n",
    "- We’ll load the saved model and tokenizer for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../Models/fine-tuned-modernbert_v2\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../Models/fine-tuned-modernbert_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert_env)",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
