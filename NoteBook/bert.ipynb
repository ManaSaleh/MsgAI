{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 1: **Load the Dataset**\n",
    "We’ll start by loading the dataset from the CSV file (`smsDataLast.csv`) using `pandas`.\n",
    "\n",
    "### Explanation:\n",
    "- The dataset contains SMS messages and their corresponding categories (labels).\n",
    "- We’ll use `pandas` to load the data into a DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../Data/smsDataLast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sender</th>\n",
       "      <th>Message Content</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maharah co.</td>\n",
       "      <td>لفترة محدودة خصم 10%\\r\\nعلى خدمة العمالة المنز...</td>\n",
       "      <td>Promotional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stc</td>\n",
       "      <td>عميلنا العزيز \\nتم اكتشاف عطل فني على هاتفكم ر...</td>\n",
       "      <td>Telecommunications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stc</td>\n",
       "      <td>عميلنا العزيز ..\\nنتشرف بخدمتكم، ونفيدكم بأنه ...</td>\n",
       "      <td>Telecommunications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mawarid</td>\n",
       "      <td>عميلنا العزيز \\r\\nتم إستلام مبلغ  109.25 عن طر...</td>\n",
       "      <td>Commercial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alrajhibank</td>\n",
       "      <td>عزيزتنا عميلة التميز، نهنئكم بعيد الأضحى المبا...</td>\n",
       "      <td>Promotional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sender                                    Message Content  \\\n",
       "0  maharah co.  لفترة محدودة خصم 10%\\r\\nعلى خدمة العمالة المنز...   \n",
       "1          stc  عميلنا العزيز \\nتم اكتشاف عطل فني على هاتفكم ر...   \n",
       "2          stc  عميلنا العزيز ..\\nنتشرف بخدمتكم، ونفيدكم بأنه ...   \n",
       "3      mawarid  عميلنا العزيز \\r\\nتم إستلام مبلغ  109.25 عن طر...   \n",
       "4  alrajhibank  عزيزتنا عميلة التميز، نهنئكم بعيد الأضحى المبا...   \n",
       "\n",
       "             Category  \n",
       "0         Promotional  \n",
       "1  Telecommunications  \n",
       "2  Telecommunications  \n",
       "3          Commercial  \n",
       "4         Promotional  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1278 entries, 0 to 1277\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Sender           1278 non-null   object\n",
      " 1   Message Content  1278 non-null   object\n",
      " 2   Category         1278 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 30.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Financial                 370\n",
       "Promotional               204\n",
       "Commercial                137\n",
       "Governmental              137\n",
       "Conferences and Events     95\n",
       "Telecommunications         90\n",
       "Education                  53\n",
       "Stores                     46\n",
       "Services                   39\n",
       "Travel                     38\n",
       "Health                     25\n",
       "Personal                   22\n",
       "Apps                       22\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of categories\n",
    "df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 2: **Preprocess the Data**\n",
    "We’ll preprocess the data to prepare it for training. This includes:\n",
    "- Mapping categories to numerical labels.\n",
    "- Splitting the data into training and testing sets.\n",
    "\n",
    "### Explanation:\n",
    "- Text classification requires numerical labels, so we’ll convert the `Category` column to numerical values.\n",
    "- We’ll split the data into training and testing sets (e.g., 80% training, 20% testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map categories to numerical labels\n",
    "df['label'] = df['Category'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      " Category\n",
      "Financial                 296\n",
      "Promotional               163\n",
      "Commercial                110\n",
      "Governmental              109\n",
      "Conferences and Events     76\n",
      "Telecommunications         72\n",
      "Education                  42\n",
      "Stores                     37\n",
      "Services                   31\n",
      "Travel                     30\n",
      "Health                     20\n",
      "Apps                       18\n",
      "Personal                   18\n",
      "Name: count, dtype: int64\n",
      "Testing set class distribution:\n",
      " Category\n",
      "Financial                 74\n",
      "Promotional               41\n",
      "Governmental              28\n",
      "Commercial                27\n",
      "Conferences and Events    19\n",
      "Telecommunications        18\n",
      "Education                 11\n",
      "Stores                     9\n",
      "Travel                     8\n",
      "Services                   8\n",
      "Health                     5\n",
      "Personal                   4\n",
      "Apps                       4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2,  # 20% of the data for testing\n",
    "    random_state=42,  # For reproducibility\n",
    "    shuffle=True,  # Shuffle the data before splitting\n",
    "    stratify=df['Category']  # Preserve the class distribution\n",
    ")\n",
    "\n",
    "# Check the distribution of categories in the training and testing sets\n",
    "print(\"Training set class distribution:\\n\", train_df['Category'].value_counts())\n",
    "print(\"Testing set class distribution:\\n\", test_df['Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1022, 4)\n",
      "Testing data shape: (256, 4)\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training data shape:\", train_df.shape)\n",
    "print(\"Testing data shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 3: **Tokenize the Data**\n",
    "We’ll tokenize the text data using the `AutoTokenizer` from Hugging Face’s `transformers` library.\n",
    "\n",
    "### Explanation:\n",
    "- Tokenization converts text into numerical input that the model can understand.\n",
    "- We’ll use the `AutoTokenizer` for ModernBERT and ensure all sequences are padded/truncated to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mana-saleh/anaconda3/envs/bert_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"Message Content\"], padding=\"max_length\", truncation=True, max_length=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the training and testing datasets\n",
    "train_dataset = train_df.apply(tokenize_function, axis=1)\n",
    "test_dataset = test_df.apply(tokenize_function, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized example: {'input_ids': [50281, 4467, 11912, 8181, 6900, 12458, 13504, 21931, 9211, 49788, 12458, 45232, 30265, 5843, 884, 6, 190, 187, 13793, 38033, 45232, 9211, 5843, 12458, 9445, 13793, 5843, 7427, 12458, 30331, 5846, 29620, 4467, 32229, 15677, 5846, 45693, 30901, 48531, 22814, 9427, 6900, 190, 187, 5843, 5846, 23207, 11912, 6900, 6463, 14062, 50282], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check the tokenized output for the first example\n",
    "print(\"Tokenized example:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: **Test the Model on a Single Row**\n",
    "Before training, we’ll test the model on a single row to ensure it works.\n",
    "\n",
    "### Explanation:\n",
    "- We’ll load the ModernBERT model and pass a single tokenized input to it.\n",
    "- This helps verify that the model and tokenizer are working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"answerdotai/ModernBERT-base\", num_labels=len(df['Category'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single row\n",
    "sample_input = tokenizer(train_df.iloc[0][\"Message Content\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=52)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class\n",
    "predicted_class_id = outputs.logits.argmax().item()\n",
    "predicted_class = df['Category'].unique()[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Governmental\n",
      "Actual class: Services\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted class:\", predicted_class)\n",
    "print(\"Actual class:\", train_df.iloc[0][\"Category\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: **Train the Model**\n",
    "We’ll train the model using the `Trainer` API from Hugging Face.\n",
    "\n",
    "### Explanation:\n",
    "- The `Trainer` API simplifies the training process by handling training loops, evaluation, and logging.\n",
    "- We’ll define training arguments (e.g., learning rate, batch size) and train the model on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1022/1022 [00:00<00:00, 22953.44 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 256/256 [00:00<00:00, 18010.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mana-saleh/anaconda3/envs/bert_env/lib/python3.12/site-packages/transformers/training_args.py:1573: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    logging_strategy=\"epoch\",     # Log metrics at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save the model at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",         # Directory for logs\n",
    "    report_to=\"all\",              # Log to all available trackers (e.g., TensorBoard, W&B)\n",
    "    load_best_model_at_end=True,  # Required for EarlyStoppingCallback\n",
    "    metric_for_best_model=\"eval_loss\",  # Use validation loss to determine the best model\n",
    "    greater_is_better=False,      # Lower validation loss is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Stop if validation loss doesn't improve for 3 epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='448' max='1280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 448/1280 03:26 < 06:26, 2.16 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.625700</td>\n",
       "      <td>1.264979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>1.070356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>1.001633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.468400</td>\n",
       "      <td>0.939692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.320400</td>\n",
       "      <td>1.036855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.191700</td>\n",
       "      <td>1.004694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.145300</td>\n",
       "      <td>1.088322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=448, training_loss=0.6509223857096263, metrics={'train_runtime': 208.3715, 'train_samples_per_second': 98.094, 'train_steps_per_second': 6.143, 'total_flos': 247606077731376.0, 'train_loss': 0.6509223857096263, 'epoch': 7.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 6: **Evaluate the Model**\n",
    "After training, we’ll evaluate the model on the test set.\n",
    "\n",
    "### Explanation:\n",
    "- We’ll use the `Trainer` API to evaluate the model’s performance on the test dataset.\n",
    "- Metrics like accuracy, precision, recall, and F1 score can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.9396919012069702, 'eval_runtime': 1.4608, 'eval_samples_per_second': 175.252, 'eval_steps_per_second': 21.906, 'epoch': 7.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75390625\n",
      "Classification Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "           Promotional       0.67      0.50      0.57         4\n",
      "    Telecommunications       0.80      0.89      0.84        27\n",
      "            Commercial       0.83      0.53      0.65        19\n",
      "              Services       0.38      0.73      0.50        11\n",
      "                Health       0.94      0.84      0.89        74\n",
      "          Governmental       0.86      0.68      0.76        28\n",
      "              Personal       1.00      0.40      0.57         5\n",
      "             Financial       0.60      0.75      0.67         4\n",
      "                Stores       0.74      0.85      0.80        41\n",
      "                  Apps       0.30      0.38      0.33         8\n",
      "Conferences and Events       0.55      0.67      0.60         9\n",
      "             Education       0.58      0.61      0.59        18\n",
      "                Travel       1.00      1.00      1.00         8\n",
      "\n",
      "              accuracy                           0.75       256\n",
      "             macro avg       0.71      0.68      0.67       256\n",
      "          weighted avg       0.79      0.75      0.76       256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get predictions for the test dataset\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = test_dataset['label']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, predicted_labels, target_names=df['Category'].unique())\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 7: **Save the Model**\n",
    "We’ll save the fine-tuned model for future use.\n",
    "\n",
    "### Explanation:\n",
    "- Saving the model allows us to reuse it without retraining.\n",
    "- We’ll save both the model and tokenizer to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../Models/fine-tuned-modernbert/tokenizer_config.json',\n",
       " '../Models/fine-tuned-modernbert/special_tokens_map.json',\n",
       " '../Models/fine-tuned-modernbert/tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"../Models/fine-tuned-modernbert\")\n",
    "tokenizer.save_pretrained(\"../Models/fine-tuned-modernbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Step 8: **Load the Fine-Tuned Model**\n",
    "To load the fine-tuned model later:\n",
    "\n",
    "### Explanation:\n",
    "- We’ll load the saved model and tokenizer for inference or further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../Models/fine-tuned-modernbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"../Models/fine-tuned-modernbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert_env)",
   "language": "python",
   "name": "bert_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
